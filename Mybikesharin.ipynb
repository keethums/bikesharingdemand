{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Import Libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pylab # Combining both PyPlot and NumPy namespaces into a single one\nimport calendar # Library for different data/time types\nimport seaborn as sn # Statistical data visualization\nfrom scipy import stats # Uses NumPy for mathematical functions\nimport missingno as msno # Detects missing values\nfrom datetime import datetime # Convers datetime as objects\nimport matplotlib.pyplot as plt # Plotting Library\nimport warnings #Alerts user of some condition in a profram\npd.options.mode.chained_assignment = None\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import Training Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyTrain = pd.read_csv(\"../input/train.csv\")\ndailyTest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First Lets See\n1. The size of the data\n2. What the data holds\n3. What variables we want to pay attention to."},{"metadata":{},"cell_type":"markdown","source":"Here we see that there are 10886 rows with 12 columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyTrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here's what the table looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyTrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the data types the data holds."},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyTrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this, we see datetime as a string (obj). Since date and time are combined, we can probably break this down. We can also see that season, holiday, working day, and weather are stored as integers. Realistically, they should be categories so we will modify them for readability.\nThis will be done by: \n1. Create new columns: date, hour, weekday, month from the datetime column\n1. Change or coerce datatype of season, holiday, workingday, and weather to a category type (from pandas).\n1. Remove datetime column as there are no use for it.\n\nAgain, this is just to look at data and seeing what uses we have from it. Actual testing will use the original train.csv dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pandas.apply applies the function to every value in a list, or series. Super useful as you don't have to use a for loop.\n\n# Creates new column \"Date\"\ndailyTrain[\"date\"] = dailyTrain.datetime.apply(lambda x:x.split()[0]) \n# 'date' 'hour'. Take only \"hour\" portion by splitting through \":\". Creates new column date.\ndailyTrain[\"hour\"] = dailyTrain.datetime.apply(lambda x:x.split()[1].split(\":\")[0])\n# Break the newly split date data into weekdays. Use day_name function to get day of the week. datetime.strptime spilts date-string into a given format. Then find the weekday.\ndailyTrain[\"weekday\"] = dailyTrain.date.apply(lambda date:calendar.day_name[datetime.strptime(date,\"%Y-%m-%d\").weekday()]) #Instance method are methods which require an object of its class to be created before it can be called. \n# Repeat weekday code for month.\ndailyTrain[\"month\"] = dailyTrain.date.apply(lambda date:calendar.month_name[datetime.strptime(date,\"%Y-%m-%d\").month]) #Class attribute are attributes which are owned by the class itself. They will be shared by all the instances of the class.\n# Use map() to apply a categorical value to every variable under the season column.\ndailyTrain[\"season\"] = dailyTrain.season.map({1: \"Spring\",2: \"Summer\",3: \"Fall\",4:\"Winter\"})\n# Same as season. Create dictionary for weather values\ndailyTrain[\"weather\"] = dailyTrain.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" })\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we can change the data type of hour, weekday, month, season, weather, holiday, and workingday into the category type.\nHoliday and working day are represented by 0 and 1, so we can categorize those as well. \nThen, we can drop the datetime column as we won't be needing that."},{"metadata":{"trusted":true},"cell_type":"code","source":"categoryVariable = [\"hour\",\"weekday\",\"month\",\"season\",\"weather\",\"holiday\",\"workingday\"]\nfor var in categoryVariable:\n    dailyTrain[var] = dailyTrain[var].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dailyTrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check to see if there are any missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.bar(dailyTrain,(10,2)) # Check to see if there are any missing data.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Outlier Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2,ncols=2)\nfig.set_size_inches(30, 10)\nsn.boxplot(data=dailyTrain,y=\"count\",orient=\"v\",ax=axes[0][0])\nsn.boxplot(data=dailyTrain,y=\"count\",x=\"season\",orient=\"v\",ax=axes[0][1])\nsn.boxplot(data=dailyTrain,y=\"count\",x=\"hour\",orient=\"v\",ax=axes[1][0])\nsn.boxplot(data=dailyTrain,y=\"count\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\n\naxes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\naxes[0][1].set(ylabel='Count',title=\"Box Plot On Count Across Season\")\naxes[1][0].set(xlabel='Hour Of The Day', ylabel='Count',title=\"Box Plot On Count Across Hour Of The Day\")\naxes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Box Plot On Count Across Working Day\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"We can see the count variable holds several outliers beyond the upper extreme. \nWhen compared to seasons, we can spring has a dip in count.\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Correlation Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatt = dailyTrain[[\"temp\",\"atemp\",\"casual\",\"registered\",\"humidity\",\"windspeed\",\"count\"]].corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nsn.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can see observe a few things above:\n* Temp and atemp shows a strong correlation to eachother. Because we want to avoid multicollinearity data, we should drop atemp.\n* Windspeed does't seem to useful at the moment.\n* Casual and registered can result to data leakage, so we should probably drop thos ewhen building the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3) = plt.subplots(ncols=3)\nfig.set_size_inches(20, 5)\nsn.regplot(x=\"temp\", y=\"count\", data=dailyTrain,ax=ax1)\nsn.regplot(x=\"windspeed\", y=\"count\", data=dailyTrain,ax=ax2)\nsn.regplot(x=\"humidity\", y=\"count\", data=dailyTrain,ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What's intresting here is there seems to be an abnormal count for 0 in windspeed. This can be contributing to the low r-value."},{"metadata":{},"cell_type":"markdown","source":"**Count against Categories**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3,ax4)= plt.subplots(nrows=4)\nfig.set_size_inches(20,20)\nsortOrder = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\nhueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n\nmonthAggregated = pd.DataFrame(dailyTrain.groupby(\"month\")[\"count\"].mean()).reset_index()\nmonthSorted = monthAggregated.sort_values(by=\"count\",ascending=False)\nsn.barplot(data=monthSorted,x=\"month\",y=\"count\",ax=ax1,order=sortOrder)\nax1.set(xlabel='Month', ylabel='Avearage Count',title=\"Average Count By Month\")\n\nhourAggregated = pd.DataFrame(dailyTrain.groupby([\"hour\",\"season\"],sort=True)[\"count\"].mean()).reset_index()\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"season\"], data=hourAggregated, join=True,ax=ax2)\nax2.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Season\",label='big')\n\nhourAggregated = pd.DataFrame(dailyTrain.groupby([\"hour\",\"weekday\"],sort=True)[\"count\"].mean()).reset_index()\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"weekday\"],hue_order=hueOrder, data=hourAggregated, join=True,ax=ax3)\nax3.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Weekdays\",label='big')\n\nhourTransformed = pd.melt(dailyTrain[[\"hour\",\"casual\",\"registered\"]], id_vars=['hour'], value_vars=['casual', 'registered'])\nhourAggregated = pd.DataFrame(hourTransformed.groupby([\"hour\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\nsn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"value\"],hue=hourAggregated[\"variable\"],hue_order=[\"casual\",\"registered\"], data=hourAggregated, join=True,ax=ax4)\nax4.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across User Type\",label='big')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lets Train the Data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = dailyTrain.append(dailyTest)\ndata.reset_index(inplace=True)\ndata.drop('index',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Append Data Set and Feature Engineer Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"date\"] = data.datetime.apply(lambda x : x.split()[0])\ndata[\"hour\"] = data.datetime.apply(lambda x : x.split()[1].split(\":\")[0]).astype(\"int\")\ndata[\"year\"] = data.datetime.apply(lambda x : x.split()[0].split(\"-\")[0])\ndata[\"weekday\"] = data.date.apply(lambda dateString : datetime.strptime(dateString,\"%Y-%m-%d\").weekday())\ndata[\"month\"] = data.date.apply(lambda dateString : datetime.strptime(dateString,\"%Y-%m-%d\").month)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assign new data-type to categories and drop data leakage variables and unused variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncategoricalFeatureNames = [\"season\",\"holiday\",\"workingday\",\"weather\",\"weekday\",\"month\",\"year\",\"hour\"]\nnumericalFeatureNames = [\"temp\",\"humidity\",\"windspeed\",\"atemp\"]\ndropFeatures = ['casual',\"count\",\"datetime\",\"date\",\"registered\"]\n\n\nfor var in categoricalFeatureNames:\n    data[var] = data[var].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting Train And Test Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataTrain = data[pd.notnull(data['count'])].sort_values(by=[\"datetime\"])\ndataTest = data[pd.notnull(data['count'])].sort_values(by=[\"datetime\"])\ndatetimecol = dataTest[\"datetime\"]\nyLabels = dataTrain[\"count\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train and Validator Split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_validate, y_train, y_validate = train_test_split( dataTrain, yLabels, test_size=0.3, random_state=42)\ndateTimeColValidate = X_validate[\"datetime\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping Unncessary Variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataTrain  = dataTrain.drop(dropFeatures,axis=1)\ndataTest  = dataTest.drop(dropFeatures,axis=1)\nX_train = X_train.drop(dropFeatures,axis=1)\nX_validate = X_validate.drop(dropFeatures,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RMSLE Scorer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_,convertExp=True):\n    if convertExp:\n        y = np.exp(y),\n        y_ = np.exp(y_)\n    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n    calc = (log1 - log2) ** 2\n    return np.sqrt(np.mean(calc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Regression Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.model_selection import GridSearchCV\n\n# Initialize logistic regression model\nlModel = LinearRegression()\n\n# Train the model\nlModel.fit(X = X_train,y = np.log1p(y_train))\n\n# Make predictions\npreds = lModel.predict(X= X_validate)\nprint (\"RMSLE Value For Linear Regression In Validation: \",rmsle(np.exp(np.log1p(y_validate)),np.exp(preds),False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets take a closer look:"},{"metadata":{},"cell_type":"markdown","source":"Above, we see theres a difference of 0.9826 between predicted and actual results. Its 'aight"},{"metadata":{"trusted":true},"cell_type":"code","source":"predsTest = lModel.predict(X=dataTest)\nfig,(ax1,ax2)= plt.subplots(ncols=2)\nfig.set_size_inches(20,5)\nsn.distplot(yLabels,ax=ax1,bins=100)\nsn.distplot(np.exp(predsTest),ax=ax2,bins=100)\nax1.set(title=\"Training Set Distribution\")\nax2.set(title=\"Test Set Distribution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So a couple of notes:\n* We can see there is a high numer of 0's in the training set distribution. \n* Some things we can do to make model better is Regularization. In this case, Overfitting and Multicollinearity can be of issue. (Lasso Regression) comes handy with overfitting by reducing the coefficients to zero there by producing simpler models. Rdge Regression can come in handy for multilinearity.\n* Ensemble. By combining diverse sets of weak models, we can come up with something new (maybe).\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}